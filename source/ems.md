#EMS 
### Table of content
- [1. Overview](#overview)
- [2. Annotation Service](#annotation)
- [3. Demo Register Service](#register)
- [4. Demo Classification Service ](#glasses-mask)
- [5. Demo Ecommerce](#ecommerce)
- [6. Pipeline](#pipeline)
- [7. Add A New Service](#new-service)
- [8. Others](#other)
- [9. Debugging](#debug)


## 1. Overview <a name=overview></a>

EMS (Eyeq Machine learning as a Service) allow user to subscribe to services. 
EMS currently has the following service:

- Annotation: Create new face dataset from uploaded video, csv file, ...
- Demo register face: Register and recognize user in web browser
- Demo classification (meta services)
    + mask-glasses: check if user is wearing glasses or mask

#### Repository
**Repository Location**: [iq_facial_recognition](https://gitlab.com/eyeq.tech/iq_facial_recognition) 

**Branch**: annotation-server
```
build_annotation.sh*
data/
Dockerfile*
enviroment.txt*
enviroment.yml*
models/
README.md*
requirements.txt*
run_all_tests.sh*
run_ecommerce.sh*
run_ems.sh*
setupenv*
source/

```

Where:

- `Dockerfile`, `enviroment.txt`, `enviroment.yml`: files for building Docker images
- `source` container all source files
- `data` container data generated by running the server
- `models`: container deep learning model
- `build_annotation.sh`: use this file to build new Docker image
- `run_ecommerce.sh`, `run_ems.sh`: helper bash file to test server

#### Start server
Use docker to start, you can also run using `run_ems.sh` or run manually using `source/ems_start_point.py` (Beware that all env variable have to be setup.

```
SUBCRIPTION_ID=5bc1ad9dd3604500133f5bab
SERVICE_TYPE=annotation # face-recognition and classification (code in service_master)
docker run --name container-${SUBCRIPTION_ID} -d -it  \
        -v $(pwd)/source://iq_facial_recognition/source \
        -v webapp-annotation-cv_image_data:/iq_facial_recognition/data/annotation \
        -v webapp-annotation-cv_export_data:/iq_facial_recognition/data/dataset \
        -v webapp-annotation-cv_video_data:/iq_facial_recognition/data/video \
        -v webapp-annotation-cv_log_data:/iq_facial_recognition/data/log \
        --network ems-net -e CV_SERVER_NAME=container-${SUBCRIPTION_ID} \
        -e SUBSCRIPTION_ID=${SUBCRIPTION_ID} \
        -e SERVICE_TYPE=${SERVICE_TYPE} annotation-server:0.1
```

Where:

- webapp-annotation-cv_image_data: contain display image (3:2) to display in webapp 
- webapp-annotation-cv_export_data: contain pikle file of the form (emb, cropped_image) where emb is the extracted feature and cropped_image is 160x160x3 image for ready for training facenet
- webapp-annotation-cv_video_data: contain uploaded video
- webapp-annotation-cv_log_data: log data for kibana with format `[%(asctime)s] [%(levelname)s] %(name)s - %(processName)s: %(message)s`, (see `source/logger.py` for more info)
- ems-net: network interface to communicate between cv-server and webapp
- CV_SERVER_NAME: also the ip address of the container in ems-net
- SERVICE_TYPE: which ems service to start, choose from `annotation`: annotation service, `face-recognition`: demo register face service, `classification`: demo classification service (see `source/config.py`)
- annotation-server:0.1: docker image


## 2. Annotation Service <a name=annotation></a>
**Location**: source/ems/annotation_server.py

**Input**: video or csv file

**Output**: face dataset of format (LFW format):

```
├── dataset_id
│   ├── image_id.pkl
```

#### API:

Re-process dataset

```
POST /reprocess
```

Delete this dataset

```
POST /delete
```

Find nearest face to some_image_id

```
POST /findNearest
Body: {'image_id': 'some_image_id` }
Return: {'data': ['nearest_image_id']}
```

#### Building block

```
URLFrameReader 
 \-> FaceDetectWorker
  \-> PreprocessDetectedFaceWorker
   \-> FaceDistributorWorker
    \-> FaceExtractWorker
     \-> FullTrackTrackingWorker
      |-> AnnotationStorageWorker
      |-> AnnotationDatabaseWorker
     
```

## 3. Demo Register Service <a name=register></a>

**Location**: source/ems/demo_register_server.py

**Input**: SocketIO Stream, register event

**Output**: Matched face id or new_face

#### API:

Register new face_id

```
POST /register 
BODY: {'face': face_object, 'client_id': 'to identify the new face come from which browser', 'sent_at': timestamp}
```

## 4. Demo classification Service <a name=glasses-mask></a>

**Location**: source/ems/demo_register_server.py

**Input**: SocketIO Stream

**Output**: Wearning glasess or not, Wearing mask or not

## 5. Demo Ecommerce <a name=ecommerce></a>

**Location**: source/ems/demo_ecommerce_server.py

**Input**: list of base64 image strings

**Output**: matched face_id

## 6. Pipeline <a name=pipeline></a>

Is a set of data processing elements connected in series, where the output of one element is the input of the next one. Inspire by (Mpipe)[https://github.com/vmlaker/mpipe], which is a small wrapper around Python `multiprocessing.Process`

**Location**: source/pipe

#### pipeline.py
Represent the pipeline and serve at entry point for input

- Create the pipeline and link to first stage

```
pipeline = pipe.Pipeline(stageDetectFace)
```

- Put task to pipeline to process

```
pipeline.put(task)
```

- Terminate pipeline

```
pipeline.put(None)
```

- Get the result from last stage as generator

```
results = pipeline.results()
result = next(results)
```

#### stage.py
Stage is the smallest step of computation that can do task independently, each stage can have multiple worker

- Declare stage with a number of worker = 1

```
stageDetectFace = pipe.Stage(pipe.FaceDetectWorker, 1)
stagePreprocess = pipe.Stage(pipe.PreprocessDetectedFaceWorker, 1)
```

- Send the additional params to the constructor of worker

```
stageTrack = pipe.Stage(pipe.FullTrackTrackingWorker, 1, area=area, database=db, matcher=matcher, init_tracker_id=number_of_existing_trackers)
```

- Link upstream stage to downstream, in this case output of `stageDetectFace` will be sent to `stagePreprocess`

```
stageDetectFace.link(stagePreprocess)
```

- Upstream stage can send the result to multiple downstreams, the example is the `stageTrack`. After tracking, we will have the recongnized trackers to send to database, storage, dashboard, ...

```
stageTrack.link(stageStorage)
stageTrack.link(stageDatabase)
stageTrack.link(stageDashboard)
```

#### worker.py <a name=worker ></a>
Basic unit that do the computation task on input, it's a subclass of `multiprocessing.Process`. 

- Init the worker with additional args. Or you can init in `doInit` function

```
def __init__(self, **args):
	database = args.get('database')
	matcher = args.get('matcher')
```

- Handle different type of task by override the following method. By default the task will be cascaded to next downstream stage and eventually come out as result

```
def doFrameTask(self, task) # Handle Task.Frame
def doFaceTask(self, task) # Handle Task.Face
def doEventTask(self, task) # Handle Task.Event
```

- Handle clean up task when get terminate signal. Example usage: tracking worker will clean up remaining trackers when the video is complete

```
def doFinish(self)
```

- Put task to next stage

```
sefl.putResult(task)
```

##### Currently use worker (can be found in `source/pipe`)
```
dashboard_worker.py*
database_worker.py*
demo_ecommerce_worker.py
demo_glasses_mask_worker.py
demo_register_worker.py
face_detect_worker.py*
face_distributor_worker.py*
face_extract_worker.py*
face_preprocess_worker.py*
frame_log_worker.py*
storage_worker.py*
tracking_worker.py*
video_worker.py*
```

where:

- `dashboard_worker.py`: Handle the recognized trackers and put the result to dashboard
- `database_worker.py`: Handle the recongized trackers and save the information into databasem, work hand in hand with `source/database.py`
- `storage_worker.py`: Hanlde the recongized trackers and store the images: either display image, cropped images or in pkl format
- `video_worker.py`: Handle the recognized trackers and sum the result up to write the detected bounding box and recognized face_id at the end 
- `frame_log_worker.py`: Handle the recognized trackers the same way as `video_worker` but write the result to a `.json` file
- `demo_ecommerce_worker.py`, `demo_glasses_mask_worker.py`, `demo_register_worker.py`: specific worker to handle ems case
- `face_detect_worker.py`: Handle frame and recognized face 
from it
- `face_distributor_worker.py`: Some frames have many faces, while other have too little, the idea behind this worker is to distribute the detected faces equally to feature extracting step without overload the resources if there are too many faces and make use of resource more efficiently by combine faces from many frame and extracted at a single step. Unfortunately, only the first idea is implemented :v 
- `face_extract_worker.py`: Extract embs from detected faces and either output `FaceInfo` with contain emb, face quality, bounding box, landmarks, ... and embedding only. These two kind of output are reflected by two diffrent implementation `FaceExtractWorker` and `EmbeddingExtractWorker`
- `face_preprocess_worker.py`: preprocess detected face, step include: cropped and apply whitening
- `tracking_worker.py`: Handle the tracking feature by two way: track by overlap and track by embedding, All the logic of recognization happen here. There are two classes: `RealTimeTrackingWorker` output the recognized tracker at soon as it has enough face for that tracke, while `FullTrackTrackingWorker` output the recognized tracker only when no more faces are added to that tracker for a particular timelapse. **Note**: The code for these TrackingWorker is still from Mẫn Hồ

#### taks.py
Wrapper around the input and output of each stage. There are 3 types of task:

- Task.Frame: Indicate this task The stage receive this task will do computation on the frame, example: detect face from frame, preprocess frame, ...
- Task.Face: Handle task with face. Example: Extract face features from list of cropped face
- Task.Event: Handle event comming without interrupt the processing of the pipeline. Example usage: Handle merge split signal, handle register signal, handle find nearest signal, .... It's useful to handle API call without interrupting the running pipeline

Packaging and depacking the task:

```
# packing
task = Task(Taskj.Frame)
task.package(frame=frame, frame_info=frame_info)

# depacking
data = task.depackage()
frame, frame_info = data['frame'], data['frame_info''
```

The idea behinds packing and depacking is that you can package more than you need (so it'll fit more use case) but depacke only stuff for your particular task.

## 7. Add new service <a name=new-service ></a>

**Note**: On setup env, use `source/ems.yml` to set up development enviroment

#### 1.Define new service in service master

There are currently 3 service: annotation, demo register, demo classification

```
{
    "name" : "Annotation Tool ",
    "description" : "Create a human face dataset from video",
    "code" : "annotation",
    "order" : 1,
    "createdAt" : ISODate("2018-09-13T13:32:25.884+07:00"),
    "updatedAt" : ISODate("2018-09-13T13:32:25.884+07:00")
}

{
    "name" : "Face Recognition",
    "description" : "Register and recognize your face",
    "code" : "face-recognition",
    "order" : 2,
    "createdAt" : ISODate("2018-09-13T13:32:25.884+07:00"),
    "updatedAt" : ISODate("2018-09-13T13:32:25.884+07:00")
}

{
    "name": "Classification",
    "description": "Classify objects to predefined sets",
    "code": "classification",
    "order": "3",
    "args": [
    {
        "name": "Classification Case",
        "key": "classificationCase",
        "type": "select",
        "valueSet": ["MASK_GLASSES_CLASSIFICATION"]
    }],
}
```

New services have to be add to service_master collections before they can be subscribed in the webapp.
Generally different services have their own record, but in some cases, if the services have the same webapp UI, we can group them into one single meta service and use the args fields to switch between them.

```
{
    "name" : "New service",
    "description" : "New serive name",
    "code" : "service-code ($SERVICE_TYPE env variable)",
    "order" : ? (Check how many service already in service_master to know),
    "args": [
    {
        "name": "Display name of this field",
        "key": "keyOfThisField",
        "type": "select|input",
        "valueSet": ["SOME_PREDEFINED_INPUT_FOR_SELECT"]
    }],
    "createdAt" : ISODate("Whatever date"),
    "updatedAt" : ISODate("2018-10-02T13:32:25.884+07:00")
}
```

Add serice by run the following command in `mongod` container
- Start mongod

```
docker exec -it mongod /bin/bash
```
- Execute

```
mongo
use annotation-prod;
db.getCollection('service_master').insert({...new service json...}); 
```

#### 2. Add configuration

- subclass `source/configuration/base_config.py` in new file `source/configuration/new_service.py` and update configuration you want, the class you may want to change most is `MongoDB`
- Update `source/config.py` for new service type, or you can just import `source/configuration/new_service` directly in you server

#### 3. Subclass `AbstractDatabase` in `/source/database.py`

Each service will interact with database quite differently, so put your database handling logic here. Most important methods is `insert_new_face` to handle add new detected face to database and `get_labels_and_embs` to build matcher

**Note**: This class is quite a mess, take time to make it better. One way to make it better is use 

#### 4. Add new worker in `source/pipe/`

Worker is the basic unit that handle task independently. Add new worker that do you specific task or reuse some of the worker already in used. The list of currently used worker can be found [here](#worker)

#### 5. Add new server in `source/ems/`

Subclass `AbstractServer` in `base_server.py` by:

- `add_endpoint`: Add new api end point to this method
- `init`: build pipeline or do what ever logic you want here
- If you use pipeline, handle API request by put new `task = Task(Task.Event)` to the pipeline `pipeline.put(task)` and handle the task in any `doEventTask` in any of the stage in the pipeline without interrupt it


#### 6. Add new service to `source/ems_start_point.py`
It is just a switch case

#### 7. Test

## Other stuff <a name=other ></a>

#### Dynamically retrieve avaiable gpu
**Location**: source/configuration/gpu.py

GPU is limited computational resource that may not avaiable when the service start to run. I currently use [GPUtils](https://github.com/anderskm/gputil) to get the first avaiable gpu for the service. 

**Some thought**: Wrap the library inside class GPU and wait until there is some avaiable gpu and start the service

#### Configuration management
Changing configuration for each project is daunting is the pass. Now the pain is over (hope so). The file `souce/configuration/base_config.py` contains the default, you can override and create new configuration file in `souce/configuration`. Take a look at serveral configuration there for inspiration. You can either `from config import Config` using `source/config.py` or `import configuration.unilever as Config` using `souce/configuration/unilever.py` directly. It's all up to you.

**One downside**: you need to export PYTHONPATH for this to work. Let's me know some other technique.

```
# Assume $(pwd)=path/to/iq_facial_recognition/source
export PYTHONPATH=
export PYTHONPATH=$(pwd)/configuration:$PYTHONPATH
export PYTHONPATH=$(pwd):$PYTHONPATH
```

## 9. Debugging <a name=debug ></a>

#### Error:
`CV_SOCKER_NOT_INITALIZED` => This mean that the the cv server has been crash or not properly initialized.

#### How to read Docker logs:
First, we need to find out what cv-container has been crashed. We have to go inside the mongod container to inspect the `subscriptionId` of the container. You can also obtain the `subscriptionId` by going to `Detail page` of the service.

```
$host docker exec -it mongod /bin/bash
$mongod mongo

# (mongo) use annotation-prod
# (mongo) db.getCollection('service-subcription').find()
```

Then, read the logs to see what crash the service. Or you can go to [https://kibana.eyeq.tech](https://kibana.eyeq.tech) to find out the problems.

```
$host docker logs container-${subcription_id}
```






